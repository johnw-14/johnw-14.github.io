---
layout: post
title: "Exploring Predictive Policing Data in Chicago"
output: 
  md_document: 
    variant: markdown_github
    preserve_yaml: true
---

```{r include = FALSE}
knitr::opts_knit$set(base.dir = "/Users/whele/Documents/johnw-14.github.io/docs/", base.url = "/")
knitr::opts_chunk$set(fig.path = "images/")
```

* * *


## Introduction

Hello! If you're interested in a very introductory look at algorithmic bias, as well as some statistics in R, you are in the right place.

This post was prompted as I read the [To Surveil and Predict](https://citizenlab.ca/2020/09/to-surveil-and-predict-a-human-rights-analysis-of-algorithmic-policing-in-canada/) report by Kate Robertson, Cynthia Khoo, and Yolanda Song at the Citizen Lab in Toronto, Canada. One of the references in the report was to a public dataset of the Chicago Police Department's Strategic Subject Algorithm, which was used until 2019. As someone interested in algorithmic bias, statistics, and building my R experience, it seemed like a great place to dig a bit deeper. 

I will not go too far into the many (!) ethical and legal concerns of law enforcement using complex algorithms. I strongly recommend accessing the report if you want to learn more. Or, if you prefer, [this webinar](https://www.youtube.com/watch?v=UyVUSu8lrYQ&ab_channel=BCCivilLibertiesAssociation) hosted by the British Columbia Civil Liberties Association may be a more accessible place to start. Kate Robertson (the primary author on the report) was one of the speakers.

My focus here will be a few statistical tests of the Chicago PD SSL algorithm's output. I'll run those tests in R, but the programming is pretty straightforward. Given that, I will focus mostly on the statistical tests and results, rather than the R code to run them. There are all kinds of R tutorials out there that can better explain getting up and running.

The dataset and more information is available here: https://data.cityofchicago.org/Public-Safety/Strategic-Subject-List-Historical/4aki-r3np


* * *


## Load Packages in R

```{r load-packages, message = FALSE}
library(tidyverse)
library(car)
library(dunn.test)
```

```{r load-data, echo = FALSE}
ChicagoPD_data <-
  read.csv("C:\\Users\\whele\\OneDrive\\Documents\\ChicagoPD\\Strategic_Subject_List_-_Historical.csv")
```


* * *


## Initial Data Analysis

The Chicago Police Department's Strategic Subject Algorithm was used "to create a risk assessment score known as the Strategic Subject List or 'SSL.' These scores reflect an individualâ€™s probability of being involved in a shooting incident either as a victim or an offender."

An immediate concern is whether this algorithm could be biased with regards to marginalized individuals. The background information on the SSL data makes it very clear that race and gender are not used as predictor variables in calculating the SSL score. However, most everything I have read on algorithmic bias brings up the fact that even if a characteristic such as race isn't explicitly used as a predictor, other predictor variables can easily be a proxy for it (address or ZIP/postal code is a frequent example, as racialized individuals often live in similar neighbourhoods for various socio-political reasons).

One of the first things I wanted to look at is simply how the mean SSL scores of each Race Code in the data set compare. The race codes used in the data set are as follows:

BLK - Black; WHI - White; API - Asian/Pacific Islander; WBH - Black Hispanic; WWH - White Hispanic; I - American Indian/Alaskan Native; U - Unknown

A quick summary of how the data is distributed over those race codes:

```{r}
ChicagoPD_data %>%
  count(RACE.CODE.CD)
```

Right away it's pretty striking to see how many Black individuals are in the arrest data set compared to the other Race Codes used. There is plenty more to discuss about that without even getting into the Strategic Subject Algorithm. But I am going to keep this post focused on the latter for now.

Along with individuals' calculated SSL scores, the dataset also has a "Raw SSL score". According to the website, this raw score was historically recorded after intermediate calculations, before being normalized into the SSL score, which is on a 500-point scale.

I wanted to see if this would affect statistical results very much. A tiny first step was to check if there are any missing values for RAW.SSL.SCORE in the data. Which there are not.

```{r}
sum(is.na(ChicagoPD_data$RAW.SSL.SCORE))
```

Let's now look at some boxplots of how the raw SSL score and SSL score are distributed among the race code groups. 

```{r rawSSL-raceCD-boxplot}
boxplot(ChicagoPD_data$RAW.SSL.SCORE ~ ChicagoPD_data$RACE.CODE.CD)
```

```{r SSL-raceCD-boxplot}
boxplot(ChicagoPD_data$SSL.SCORE ~ ChicagoPD_data$RACE.CODE.CD)
```


As one would probably expect, there appear to be some differences in distributions by race code group when looking at both raw and final SSL values. One way to check if this is statistically significant is to run an analysis of variance (ANOVA) test.


* * *


## Analysis of Variance (ANOVA)

Let's run an ANOVA test on the raw SSL score first. Formally, our hypotheses would be:

Null hypothesis (H~0~): The mean raw SSL score for each race code group is equal.

Alternative hypothesis (H~A~): At least one of the race code groups has a different mean raw SSL score from the others.

```{r}
raw_SSL_aov <- aov(RAW.SSL.SCORE ~ RACE.CODE.CD, ChicagoPD_data)
summary(raw_SSL_aov)
```

We'll consider this at 95% significance. In other words, we are comfortable with a 5% probability of falsely rejecting the null hypothesis (a quick Google search can tell you much, much more about significance levels and p-values). This test gives a p-value of less than 2x10^-16^, which is very small indeed. This indicates strong statistical evidence for rejecting the null hypothesis in favour of the alternative hypothesis that at least one of the race code groups has a different mean raw SSL score from the others.

I separately ran an ANOVA test (as well as the rest of the tests in this post) using the final SSL score rather than the raw one. Each test had nearly identical results. This makes sense given the fields' descriptions, where it looks like the final SSL score has just been normalized to a 500-point scale. Given how tightly the results match, I will outline further tests on the raw SSL score only.

Before we start parsing what the ANOVA results could mean, it is vital that we check the conditions for an ANOVA test. If any of these assumptions are violated, we cannot rely on the above result.


* * *


## Check ANOVA Assumptions

We need to check:

1. Each population distribution is normal
2. The population variances are equal
3. The samples are independent and random

To check the normality assumption, we can use the residuals in the ANOVA analysis we already ran. The residuals are the difference between each individual raw SSL score and the overall mean raw SSL score. To meet the condition, we want to see a normal distribution in residuals about a mean of zero. I'll generate a histogram and normal probability plot for this:

```{r ANOVA-residuals-histogram}
raw_SSL_aov_residuals <- residuals(object = raw_SSL_aov)
hist(x = raw_SSL_aov_residuals)
mean(raw_SSL_aov_residuals)
```

```{r ANOVA-residuals-normplot}
qqnorm(raw_SSL_aov_residuals, xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
qqline(raw_SSL_aov_residuals)
```

We see the histogram is a bit skewed, but not so much that it is an obvious problem. This also bears out in the normal probability plot, where we see deviation from normality (the straight line in this case) at the high end of the residual range. This matches the "squashed" right tail of the histogram. So all that considered, we would probably be safe considering the population distrbution to be near normal.

Next, we want to check that the groups examined have a constant variance in raw SSL score. For this I will use Bartlett's test, where:

H~0~: the variances of the groups are equal

H~A~: at least one of the groups has a different variance from the others

```{r}
variance.test <- bartlett.test(RAW.SSL.SCORE ~ RACE.CODE.CD, ChicagoPD_data)
variance.test
```

The p-value for this test is also extremely low. This would indicate rejecting the null hypothesis in favour of the alternative hypothesis that the variances are NOT equal. Which is an issue for our ANOVA assumptions. Let's quickly try an alternative, Levene's test, which is more conservative (i.e. less likely to falsely reject the null hypothesis).

```{r}
leveneTest(RAW.SSL.SCORE ~ RACE.CODE.CD, ChicagoPD_data)
```

This result also shows strong statistical evidence that the variances in raw SSL score for each race code are not equal. This is strong evidence that we do not meet this assumption.

The final assumption we need to discuss is that the observations are independent and random. The data uses the Chicago PD's full list of arrested individuals between August 1, 2012 to July 31, 2016. As such, the population we are examining is arrested individuals in that timeframe. Rather than having a random sample from the population, we have the full population to work with. So in terms of ANOVA assumptions, I don't think there is an issue here. 

Independence refers to whether any observed values of the SSL score could affect one another. We would need to know more about the algorithm and its deployment to be certain of this. But it's not hard to imagine an example where high SSL scores in one community lead to increased policing of that community, leading to arrests of other individuals in the database, which in turn raise their own SSL scores. Especially since we are dealing with arrest data over a few years. So I would certainly have concerns around the independence of the observations.

Given the issues with equal variance and independence, it looks like the conditions for the ANOVA test we ran are violated.


* * *


## Non-Parametric Alternative to ANOVA

Let's try using a non-parametric test, which does not require the same conditions on the groups' distributions be met. We'll use the Kruskal-Wallis H test.

H~0~: the distributions of raw SSL score for each race code group is the same.

H~A~: at least one of the race code groups has a raw SSL score distribution that is different from the others.

```{r}
kruskal.test(RAW.SSL.SCORE ~ RACE.CODE.CD, ChicagoPD_data)
```

Again, we have very strong statistical evidence to reject the null hypothesis in favour of the alternative hypothesis that at least two of the race codes have different distributions of raw SSL score. 

Let's now use a non-parametric multiple comparisons test to try to identify which specific race codes are different from one another. We do this using Dunn's test, which comes from a separate package. See its documentation [here](https://cran.r-project.org/web/packages/dunn.test/dunn.test.pdf).

```{r}
dunn.test(ChicagoPD_data$RAW.SSL.SCORE, ChicagoPD_data$RACE.CODE.CD, method = "bonferroni")
```

The table above represents the comparisons between each Race Code group. A reminder on the abbreviations:

BLK - Black; WHI - White; API - Asian/Pacific Islander; WBH - Black Hispanic; WWH - White Hispanic; I - American Indian/Alaskan Native; U - Unknown

So the top left values are for the comparison of Asian/Pacific Islander raw SSL score against Black individuals in the dataset. The cell below it compares Asian Pacific Islander raw SSL score with American Indian/Alaskan Native, and so on. The values in each cell are Dunn's pairwise z test statistic, and its associated adjusted p-value. Without digging too deeply into these concepts, the things we are most interested in here are really the sign of the z-score and whether or not the p-value has an asterisk beside it.

Starting with the p-value, the output flags any statistically significant results (based on our instructions) with an asterisk. We see this with the very first comparison in the table. It is also important to look at the "direction" of this difference. For example, the negative z-score for the API/BLK comparison indicates that the API raw SSL score distribution falls "below" or "to the left of" the BLK distribution. And we know from the p-value that this is statistically significant.

With that context, here are a few takeaways from the test:

-We see that, according to the statistical evidence, the distribution of raw SSL scores for Black individuals is to the right (i.e. generally higher scores) than every other Race Code group except for Black Hispanic individuals.

-We also see evidence that the Black Hispanic group's distribution is to the right of all other groups, with the aforementioned exception of the Black code group, as well as White Hispanic. This is also important to note from an intersectional lens, where we fail to identify any significant difference between Black and Black Hispanic individuals, as well as Black Hispanic and White Hispanic groups. This is another concerning indication of how the SSL score behaves with regards to race.

-Conversely, the Asian/Pacific Islander group has statistically significant evidence for a distribution to the left of all other groups except White and American Indian/Alaskan Native (important to note there are only 276 individuals in the latter group).

-And similarly, we see evidence that the White code group has an SSL distribution to the left of all other code groups, with the aformentioned exception of the Asian/Pacific Islander and American Indian/Alaskan Native code groups.

The results appear to show many significant differences in SSL score distributions amongst the race code groups. This is notable, because our initial Kruskal-Wallis test results could have simply been indicating one outlying distribution. But there appear to be marked differences throughout.


* * *


## Conclusion

This was a very surface-level statistical look at a project with many technical, legal and socio-political concerns. As referenced in the introduction, the [To Surveil and Predict](https://citizenlab.ca/2020/09/to-surveil-and-predict-a-human-rights-analysis-of-algorithmic-policing-in-canada/) report by Kate Robertson, Cynthia Khoo, and Yolanda Song is a great place to start if you want to dig more deeply into these issues.

From an introductory statistical perspective, it is quickly apparent that the SSL scores assigned by the algorithm (purported to identify individuals with a higher probablity of being involved in a shooting, as a victim or offender) are significantly different between various racial groups. This is in spite of the fact that the algorithm was designed without race or gender being directly used as input variables.

There are likely many overlapping reasons for these results. We saw there is a disproportionate amount of racialized individuals in the arrest data to begin with, as is consistently the case in North America (an issue far beyond the scope of one blog post). Further examination would be needed to see how the algorithm interacts with this underlying data. Perhaps it helps mitigate the biased data, or perhaps it amplifies the effect.

Regardless, you can quickly see how increased policing of individuals with high SSL scores, which are primarily racialized individuals, would lead to increased arrests of those individuals and their communities. This would increase the amount of racialized individuals included in the arrest data, driving future high SSL scores. The cycle then repeats itself. Such "feedback loops" are a prominent concern when discussing algorithmic bias.

I will leave the discussion there for now. If you have any feedback, please don't hesitate to contact me at the site email: john.wheler.site@gmail.com. Thanks for reading.